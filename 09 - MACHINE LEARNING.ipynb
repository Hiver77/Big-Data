{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "09 - MACHINE LEARNING.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hiver77/Big-Data/blob/master/09%20-%20MACHINE%20LEARNING.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NR0mBQLzQSE3"
      },
      "source": [
        "!wget --no-cache -O init.py -q https://raw.githubusercontent.com/UDEA-Esp-Analitica-y-Ciencia-de-Datos/EACD-03-BIGDATA/master/init.py\n",
        "import init; init.init(force_download=False); \n",
        "from IPython.display import Image #hola"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjpVTH9sQZvB"
      },
      "source": [
        "Image(\"local/imgs/udea-datascience.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2k6dcyt7TIA_"
      },
      "source": [
        "#Instalación\n",
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.0.1/spark-3.0.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.0.1-bin-hadoop3.2.tgz \n",
        "!pip install -q findspark\n",
        "\n",
        "#Variables de Entorno\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.1-bin-hadoop3.2\"\n",
        "\n",
        "#SparkContext\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkContext\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "sc = SparkContext.getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDE-1bfROwk8"
      },
      "source": [
        "#SQL Context\n",
        "from pyspark.sql import SQLContext\n",
        "from pyspark.sql import functions\n",
        "from pyspark.sql.types import *\n",
        "sqlCtx = SQLContext(sc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjFB5U9fQdU1"
      },
      "source": [
        "#**MACHINE LEARNING**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yM8n4bwZTLoW"
      },
      "source": [
        "El Machine Learning o aprendizaje automático es una disciplina orientada a crear sistemas que puedan aprender por sí solos, con el fin de extraer información no trivial de grandes volúmenes de datos por medio de la identificación de patrones complejos.\n",
        "\n",
        "Spark implementa el aprendizaje automático a través del módulo MLLib que cuenta con un gran número de algoritmos que permiten crear modelos para el aprendizaje automático. Teniendo en cuenta que Spark proporciona sistemas distribuidos para trabajar en paralelo, los algoritmos de Machine Learning implementados en MLLib deben poder ser paralelizables\n",
        "\n",
        "Pueden identificarse dos grandes ramas en el aprendizaje automático, a saber, el aprendizaje supervisado y el aprendizaje NO supervisado.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZkOCJIkTPzc"
      },
      "source": [
        "##**Preparación de Datos**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeQve4QSRoQF"
      },
      "source": [
        "Para poder aplicar los métodos del aprendizaje supervisado y no supervisado, es necesario realizar una etapa previa de preparación de datos.\n",
        "\n",
        "Esta etapa nos permite identificar y corregir posibles anomalías presentes en los datos y contribuir a la etapa de aprendizaje para obtener un buen resultado en la evaluación de los modelos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkLyWg0dXqTw"
      },
      "source": [
        "###**Pasos**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byDwYTw5U-Pd"
      },
      "source": [
        "Para la preparación de datos se recomienda realizar los siguientes pasos\n",
        "\n",
        "**1. Identificación de variables**\n",
        "\n",
        "* Propiedades del conjunto de datos\n",
        "* Tipos de datos (verificar carga correcta de datos)\n",
        "\n",
        "\n",
        "**2. Tratamiento de duplicados**\n",
        "\n",
        "* Eliminar variables duplicadas (columnas)\n",
        "* Eliminar registros duplicados (filas)\n",
        "* Eliminar variables irrelevantes (ID, cedula, nombre, teléfono)\n",
        "\n",
        "\n",
        "**3. Análisis univariable**\n",
        "\n",
        "*  Variables numéricas: estadística descriptiva, histogramas, box plot\n",
        "*  Variables categóricas: tabla de frecuencias y diagrama de barras\n",
        "\n",
        "\n",
        "**4. Análisis bivariable**\n",
        "\n",
        "* Correlaciones entre las variables predictoras deben ser menores a 0.7\n",
        "* Correlaciones con la variable objetivo debe ser mayor a 0.3\n",
        "\n",
        "\n",
        "**5. Tratamiento de outliers** (eliminar registros, eliminar variables, imputar, predecir)\n",
        "\n",
        "\n",
        "**6. Tratamiento de datos nulos** (eliminar registros, eliminar variables, imputar o predecir)\n",
        "\n",
        "\n",
        "**7. Transformación de variables desde las reglas del negocio**\n",
        "\n",
        "* Discretización o Binning: convertir de número a categoría\n",
        "* Crear variables Dummy: convertir de categoría a número\n",
        "\n",
        "\n",
        "**8. Creación de variables** (fecha y otros)\n",
        "\n",
        "\n",
        "**9. Reducción de variables** (en caso de ser necesario)\n",
        "\n",
        "\n",
        "**10. Balanceo de la variable objetivo** (sólo en clasificación)\n",
        "\n",
        "\n",
        "**11. Transformación datos para el método**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYef2aspXuC8"
      },
      "source": [
        "###**Modo Local**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7THtAvpZk2a"
      },
      "source": [
        "Veamos un ejemplo de cómo realizar estos pasos de la preparación de datos en un ambiente local utilizando pandas (no es un ambiente distribuido)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zC4b-zAGIWN"
      },
      "source": [
        "import pandas as pd \n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2bCkVO3HN0y"
      },
      "source": [
        "En el archivo aprobacion_curso.xlsx se encuentran los datos de estudiantes de un curso de universidad, para cada estudiante se tienen los siguientes datos:\n",
        "* Id: Identificación del estudiante\n",
        "* Año-Semestre: Momento en el que toma el curso\n",
        "* Felder: Estilo de aprendizaje del estudiante\n",
        "* Examen de admisión\n",
        "* Nota final\n",
        "* Aprobó: (Si/No)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4O1VHp0zxleA"
      },
      "source": [
        "**1.\tIdentificación de variables**\n",
        "* Propiedades del conjunto de datos\n",
        "* Tipos de datos (verificar carga correcta de datos)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWW8tQSsuYI9"
      },
      "source": [
        "data = pd.read_excel(\"local/data/aprobacion_curso.xlsx\",sheet_name=0) #leer la primera hoja del archivo\n",
        "type(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqhbIhb0FmJm"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pzymM8ltPKT"
      },
      "source": [
        "data.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhGbzZIgGHyw"
      },
      "source": [
        "Las variables tipo Object las convertimos a categorías"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PidOBDs6vI16"
      },
      "source": [
        "data['Año - Semestre']=data['Año - Semestre'].astype('category')\n",
        "data['Felder']=data['Felder'].astype('category')\n",
        "data['Aprobo']=data['Aprobo'].astype('category')\n",
        "data.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgLj8guGpjVy"
      },
      "source": [
        "**2.\tTratamiento de duplicados**\n",
        "\n",
        "* Eliminar variables duplicadas (columnas)\n",
        "* Eliminar registros duplicados (filas)\n",
        "* Eliminar variables irrelevantes (ID, cedula, nombre, teléfono)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gm3P9BAKI4XS"
      },
      "source": [
        "**Variables Duplicadas:** Las variables Nota Final y Aprobó están duplicadas (representan la misma información), a partir de la nota final podemos saber si un estudiante aprueba o no el curso"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKyIJkxfIte0"
      },
      "source": [
        "data = data.drop('Nota_final',axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLHOs_JFJYeK"
      },
      "source": [
        "**Registros duplicados:** Dos registros que tengan el mismo Id están duplicados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gA5rdmE5J6Ja"
      },
      "source": [
        "data=data.drop_duplicates(['ID']) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2RqzhbAIk29"
      },
      "source": [
        "**Variables Irrelevantes**: Detectamos dos variables irrelevantes: Id y Año-Semestre"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-EFtt2-TIpOy"
      },
      "source": [
        "data = data.drop('ID',axis=1)\n",
        "data = data.drop('Año - Semestre',axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_UFkLgTKC9M"
      },
      "source": [
        "Así quedan los datos luego de este paso"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofCKbHD4yMiy"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWmpzJD_xteS"
      },
      "source": [
        "**3.\tAnálisis univariable**\n",
        "\n",
        "* Variables numéricas: estadística descriptiva, histogramas, box plot\n",
        "* Variables categóricas: tabla de frecuencias y diagrama de barras\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZW89l2-Y95o"
      },
      "source": [
        "Variables numéricas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_E2pjNztPx1"
      },
      "source": [
        "data.describe() #Solo para datos numéricos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZNav0w1xPii"
      },
      "source": [
        "data.plot.box()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kf8EB0kcZBOq"
      },
      "source": [
        "Variables categóricas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPKi3ffLnrNB"
      },
      "source": [
        "pd.value_counts(data[\"Felder\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDXQvKvu3KQS"
      },
      "source": [
        "pd.value_counts(data[\"Aprobo\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIMjePza3TFI"
      },
      "source": [
        "plt.subplot(1,2,1)\n",
        "pd.value_counts(data[\"Felder\"]).plot(kind=\"bar\")\n",
        "plt.subplot(1,2,2)\n",
        "pd.value_counts(data[\"Aprobo\"]).plot(kind=\"bar\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDA8JMX5ytTG"
      },
      "source": [
        "**4.\tAnálisis bivariable**\n",
        "\n",
        "* Correlaciones entre las variables predictoras deben ser menores a 0.7\n",
        "* Correlaciones con la variable objetivo debe ser mayor a 0.3\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5m_9sM7ivzZS"
      },
      "source": [
        "data.corr()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYuTjv5OZRLF"
      },
      "source": [
        "La correlación se aplica sobre variables numéricas, debemos convertir las variables categóricas a numéricas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7R9edonqbQ_"
      },
      "source": [
        "data_num=data #Hacemos una copia de los datos\n",
        "\n",
        "dummiesFelder = pd.get_dummies(data['Felder'])  #Creamos variables dummy para convertir  las categorías a números\n",
        "data_num = data_num.drop('Felder', axis=1)      #Elimino la columna original\n",
        "data_num = data_num.join(dummiesFelder)         #Adiciono las dummies\n",
        "\n",
        "data_num.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7LhBhVk9zzj"
      },
      "source": [
        "#Se codifican las categorias de la variable objetivo\n",
        "data_num[\"Aprobo\"]=data_num[\"Aprobo\"].replace({\"si\": 1, \"no\": 0})\n",
        "data_num.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSA1JogwZ4Iu"
      },
      "source": [
        "Ahora nuestras variables son numéricas, ya podemos aplicar la correlación\n",
        "* Correlación entre las variables predictoras debe ser menor de 0.7\n",
        "* Correlación con la variable objetivo debe ser mayor a 0.3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvnOQwMm-Byr"
      },
      "source": [
        "data_num.corr()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4pnYbPYqRtA"
      },
      "source": [
        "**5.\tTratamiento de outliers**\n",
        "(eliminar registros, eliminar variables, imputar, predecir)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBMez8JWTlTm"
      },
      "source": [
        "data.describe()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rNrB6PjaWNc"
      },
      "source": [
        "Si un estudiante perdió el examen de admisión (nota<3) no debería estar en el curso"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjxXK6-EOp7I"
      },
      "source": [
        "data[data[\"Examen_admisión\"]<3] #Consultamos los registros de los outliers\n",
        "data1=data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DF_LbFixPLxK"
      },
      "source": [
        "#Imputación por la media\n",
        "media=data[\"Examen_admisión\"].mean()\n",
        "data.Examen_admisión[data[\"Examen_admisión\"]<3]=media\n",
        "data.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imSBI4kWqN3c"
      },
      "source": [
        "**6.\tTratamiento de datos nulos** \n",
        "(eliminar registros, eliminar variables, imputar o predecir)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4h_I3bp9dyZC"
      },
      "source": [
        "data.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAYNOF8VdneQ"
      },
      "source": [
        "Hay dos registros con examen de admisión en null"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6ViquQrSFWr"
      },
      "source": [
        "#Imputación por la media\n",
        "data['Examen_admisión']=data['Examen_admisión'].fillna(value=data['Examen_admisión'].mean())\n",
        "data.info()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcX76zGQqdqW"
      },
      "source": [
        "**7.\tTransformación de variables desde las reglas del negocio** \n",
        "\n",
        "* Discretización o Binning: convertir de número a categoría\n",
        "* Crear variables Dummy: convertir de categoría a número\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sxHXkwMd7xa"
      },
      "source": [
        "El examen de admisión que es una variable numérica, podríamos discretizarla para volverla categórica"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqO2Bw0rQob1"
      },
      "source": [
        "pd.cut(data[\"Examen_admisión\"],3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pcf1GEMzePNW"
      },
      "source": [
        "El estilo de aprendizaje que es una variable categórica, la podemos volver numérica"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGabGsHzWctm"
      },
      "source": [
        "pd.get_dummies(data['Felder']).head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "staG8kLNqjuo"
      },
      "source": [
        "**8.\tCreación de variables** \n",
        "(fecha y otros)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwBYRWkLeZOQ"
      },
      "source": [
        "Como ejemplo podemos tomar una variable date time y extraer sus componentes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54VNYRV1BMwP"
      },
      "source": [
        "from datetime import datetime\n",
        "\n",
        "ahora = datetime.now()  # Obtiene fecha y hora actual\n",
        "print(\"Fecha y Hora:\", ahora)  # Muestra fecha y hora\n",
        "print(\"Fecha y Hora UTC:\",ahora.utcnow())  # Muestra fecha/hora UTC\n",
        "print(\"Día:\",ahora.day)  # Muestra día\n",
        "print(\"Mes:\",ahora.month)  # Muestra mes\n",
        "print(\"Año:\",ahora.year)  # Muestra año\n",
        "print(\"Hora:\", ahora.hour)  # Muestra hora\n",
        "print(\"Minutos:\",ahora.minute)  # Muestra minuto\n",
        "print(\"Segundos:\", ahora.second)  # Muestra segundo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iv-fxmyfqnaT"
      },
      "source": [
        "**9.\tReducción de variables** \n",
        "(en caso de ser necesario)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89RiyLxXe_2D"
      },
      "source": [
        "Temporalmente eliminamos la variable objetivo para analizar solo las variables predictoras que son en las que debemos aplicar la reducción de variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRBRATD4B-vw"
      },
      "source": [
        "X=data_num.drop('Aprobo', axis=1)\n",
        "#Eliminamos los nulos en la variable Examen admision\n",
        "X['Examen_admisión']=X['Examen_admisión'].fillna(value=X['Examen_admisión'].mean())\n",
        "X.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jco0z3EhfO6I"
      },
      "source": [
        "PCA o análisis de componentes principales nos permite convertir nuestras variables originales en un nuevo y reducido grupo de variables que representen a los datos originales con una menor dimensión"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywCwNgJXLDMZ"
      },
      "source": [
        "#Seleccionar la cantidad de componentes\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA().fit(X)\n",
        "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
        "plt.xlabel('number of components')\n",
        "plt.ylabel('cumulative explained variance');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwSHYDurER1t"
      },
      "source": [
        "#Aplicamos PCA para reducción de variables\n",
        "pca = PCA(n_components=5)\n",
        "principalComponents = pca.fit(X)\n",
        "print(\"Componentes principales\")\n",
        "print(pca.components_)\n",
        "print(\"Varianza acumulada\")\n",
        "varianza = pca.explained_variance_ratio_\n",
        "var_acum= np.cumsum(varianza)\n",
        "print(var_acum)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsgOROaRqx8x"
      },
      "source": [
        "**10.\tBalanceo de la variable objetivo**\n",
        "(sólo en clasificación) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1aKLk62PCGU"
      },
      "source": [
        "pd.value_counts(data[\"Aprobo\"]).plot(kind=\"bar\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rL0wdVEhgD4-"
      },
      "source": [
        "La variable objetivo (Aprobó) no cuenta con igual número de registros por categoría"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nqo_wPLPPFQU"
      },
      "source": [
        "from imblearn.over_sampling import SMOTENC\n",
        "\n",
        "sm = SMOTENC(random_state=42, categorical_features=[0]) #sm = SMOTE(random_state=42) para datos sólo numéricos\n",
        "X_bal, Y_bal = sm.fit_resample(data[['Felder', 'Examen_admisión']], data[\"Aprobo\"])\n",
        "# Creamos un dataframe con los resultados\n",
        "data_bal = pd.DataFrame({\"Felder\": X_bal[:,0], \"Examen_admisión\":X_bal[:,1], \"Aprobo\": Y_bal})\n",
        "# Veamos si ahora si está balanceado\n",
        "pd.value_counts(data_bal[\"Aprobo\"]).plot(kind=\"bar\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "roKCRBm_onK9"
      },
      "source": [
        "#Guardamos los dataFrame preparados\n",
        "data.to_excel('local/data/aprobacion_curso_pre.xlsx')\n",
        "data_bal.to_excel('local/data/aprobacion_curso_bal.xlsx',sheet_name='Data_bal')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8TxRTkmXwRE"
      },
      "source": [
        "###**Modo Distribuido (Spark)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfBg6-UWX4ZA"
      },
      "source": [
        "Ahora veamos que nos ofrece Spark para realizar la preparación de datos en modo distribuido. Nos apoyaremos en los métodos de DataFrame y en la librería de Machine Learning\n",
        "\n",
        "https://spark.apache.org/docs/latest/api/python/pyspark.ml.html\n",
        "\n",
        "https://spark.apache.org/docs/latest/ml-guide.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XqCZk5f5iJpf"
      },
      "source": [
        "data = pd.read_excel(\"local/data/aprobacion_curso.xlsx\",sheet_name=0)\n",
        "estDF=sqlCtx.createDataFrame(data)\n",
        "type(estDF)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZXUqWh6iJpb"
      },
      "source": [
        "**Identificación de variables**\n",
        "* Propiedades del conjunto de datos\n",
        "* Tipos de datos (verificar carga correcta de datos)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiA_ZRzFjKSi"
      },
      "source": [
        "estDF.printSchema()\n",
        "estDF.show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLxiCDzBntjh"
      },
      "source": [
        "En pandas nos muestra información sobre los registros que tienen valores null, DataFrames no nos muestra esa información y a demás las variables categóricas las trabajamos como String"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89CzUbVliJpw"
      },
      "source": [
        "**Tratamiento de duplicados**\n",
        "\n",
        "* Eliminar variables duplicadas (columnas)\n",
        "* Eliminar registros duplicados (filas)\n",
        "* Eliminar variables irrelevantes (ID, cedula, nombre, teléfono)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCPGeZCAoO84"
      },
      "source": [
        "estDF=estDF.dropDuplicates(['ID'])\n",
        "estDF=estDF.drop('ID','Año - Semestre','Nota_final')\n",
        "estDF.printSchema()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tarUkBP-iJqA"
      },
      "source": [
        "**Análisis univariable**\n",
        "\n",
        "* Variables numéricas: estadística descriptiva, histogramas, box plot\n",
        "* Variables categóricas: tabla de frecuencias y diagrama de barras\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvFMIRqBiJqA"
      },
      "source": [
        "Variables numéricas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSF0JbLXxA_W"
      },
      "source": [
        "estDF.select('Examen_admisión').describe().show()\n",
        "estDF.select('Examen_admisión').distinct().show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZfQH0jyzI3G"
      },
      "source": [
        "Encontró problemas con la variable examen de admisión, no puede calcular la estadística descriptiva de la variable hasta no resolver esos problemas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krxFQOfuVIjh"
      },
      "source": [
        "estDF=estDF.fillna(3.0,'Examen_admisión')\n",
        "estDF.select('Examen_admisión').describe().show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uVzsq7ciJqI"
      },
      "source": [
        "Variables categóricas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6MlrZ-m1VUD"
      },
      "source": [
        "estDF.groupBy('Felder').count().show()\n",
        "estDF.groupBy('Aprobo').count().show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqn9oGeniJqV"
      },
      "source": [
        "**Análisis bivariable**\n",
        "\n",
        "* Correlaciones entre las variables predictoras deben ser menores a 0.7\n",
        "* Correlaciones con la variable objetivo debe ser mayor a 0.3\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQmMIG4xiJqZ"
      },
      "source": [
        "La correlación se aplica sobre variables numéricas, debemos convertir las variables categóricas a numéricas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhIGlYP0-uW1"
      },
      "source": [
        "estNum=estDF\n",
        "estNum.select('Felder').distinct().show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTo-mYx88q8t"
      },
      "source": [
        "from pyspark.sql.types import *\n",
        "estNum=estNum.\\\n",
        "  withColumn('equilibrio',(estNum.Felder=='equilibrio').cast(IntegerType())).\\\n",
        "  withColumn('activo',(estNum.Felder=='activo').cast(IntegerType())).\\\n",
        "  withColumn('visual',(estNum.Felder=='visual').cast(IntegerType())).\\\n",
        "  withColumn('reflexivo',(estNum.Felder=='reflexivo').cast(IntegerType())).\\\n",
        "  withColumn('sensorial',(estNum.Felder=='sensorial').cast(IntegerType())).\\\n",
        "  withColumn('intuitivo',(estNum.Felder=='intuitivo').cast(IntegerType())).\\\n",
        "  withColumn('secuencial',(estNum.Felder=='secuencial').cast(IntegerType())).\\\n",
        "  withColumn('verbal',(estNum.Felder=='verbal').cast(IntegerType())).\\\n",
        "  withColumn('AproboInt',(estNum.Aprobo=='si').cast(IntegerType()))\n",
        "estNum=estNum.drop('Felder','Aprobo')\n",
        "\n",
        "estNum.printSchema()\n",
        "estNum.show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVun2JwgiJqd"
      },
      "source": [
        "Ahora nuestras variables son numéricas, ya podemos aplicar la correlación\n",
        "* Correlación entre las variables predictoras debe ser menor de 0.7\n",
        "* Correlación con la variable objetivo debe ser mayor a 0.3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sriF9vEMzvj5"
      },
      "source": [
        "Tenemos el método corr que calcula la correlación entre dos variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GvBQ1oAB3KQ"
      },
      "source": [
        "estNum.corr('equilibrio','activo')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1Aouwdjz5XU"
      },
      "source": [
        "La librería ml.stat cuenta con un método para calcular la correlación pero requiere definir los datos dentro de un vector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbXC7MFfDWQa"
      },
      "source": [
        "from pyspark.ml.stat import Correlation\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "\n",
        "vector_col = \"corr_features\"\n",
        "assembler = VectorAssembler(inputCols=estNum.columns, outputCol=vector_col)\n",
        "df_vector = assembler.transform(estNum).select(vector_col)\n",
        "\n",
        "matrix = Correlation.corr(df_vector, vector_col)\n",
        "\n",
        "matrix.collect()[0][\"pearson({})\".format(vector_col)].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Znu-8w6DiJqh"
      },
      "source": [
        "**Tratamiento de outliers y de nulos**\n",
        "(eliminar registros, eliminar variables, imputar, predecir)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDnKoEZ7iJql"
      },
      "source": [
        "Si un estudiante perdió el examen de admisión (nota<3) no debería estar en el curso"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqBfccCbHY7c"
      },
      "source": [
        "prom=estNum.agg(functions.avg(estNum.Examen_admisión)).take(1)[0][0]\n",
        "prom\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "994Ecy4MZsIO"
      },
      "source": [
        "estNum = estNum.withColumn('Examen_admisión', functions.when(estNum.Examen_admisión<3,prom)\\\n",
        "                                                 .when(estNum.Examen_admisión>=3, estNum.Examen_admisión))\n",
        "estNum.describe().show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJkVKu6oiJq0"
      },
      "source": [
        "**Transformación de variables desde las reglas del negocio** \n",
        "\n",
        "* Discretización o Binning: convertir de número a categoría\n",
        "* Crear variables Dummy: convertir de categoría a número\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BqL-DOAiJq1"
      },
      "source": [
        "El examen de admisión que es una variable numérica, podríamos discretizarla para volverla categórica"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SkW75Apj1ot"
      },
      "source": [
        "from pyspark.ml.feature import QuantileDiscretizer\n",
        "\n",
        "discretizer = QuantileDiscretizer(numBuckets=3, inputCol=\"Examen_admisión\", outputCol=\"ExamenCat\")\n",
        "\n",
        "result = discretizer.fit(estNum).transform(estNum)\n",
        "result.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hWQO9pq3AEV"
      },
      "source": [
        "La variable Aprobó es categórica, podemos convertirla a númerica"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9axH3iQ3cWg"
      },
      "source": [
        "from pyspark.ml.feature import StringIndexer\n",
        "indexer = StringIndexer(inputCol=\"Aprobo\", outputCol=\"AproboNum\")\n",
        "indexer.fit(estDF).transform(estDF).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1US5DjWiJrH"
      },
      "source": [
        "**Reducción de variables** \n",
        "(en caso de ser necesario)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvOSIOel65cH"
      },
      "source": [
        "estNum.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBfkE4YO7vkm"
      },
      "source": [
        "Creamos un vector que contiene las variables predictoras (eliminamos la variable objetivo)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1qtmxdomg0v"
      },
      "source": [
        "# convert to vector column first\n",
        "vector_col = \"features\"\n",
        "assembler = VectorAssembler(inputCols=estNum.drop('AproboInt').columns, outputCol=vector_col)\n",
        "vest = assembler.transform(estNum)\n",
        "df_vector = vest.select(vector_col)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqPfHkVpmLMz"
      },
      "source": [
        "from pyspark.ml.feature import PCA\n",
        "from pyspark.mllib.linalg import Vectors\n",
        "\n",
        "pca = PCA(k=3, inputCol=vector_col, outputCol=\"pcaFeatures\")\n",
        "model = pca.fit(df_vector)\n",
        "result = model.transform(df_vector).select(\"pcaFeatures\")\n",
        "result.show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKu6ST0TTSnP"
      },
      "source": [
        "##**Aprendizaje Supervisado**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOXtIaKMa-ke"
      },
      "source": [
        "El aprendizaje supervisado utiliza un conjunto histórico de datos donde se tienen los registros previamente catalogados, para crear un modelo de predicción. Este modelo de predicción aprende de los datos históricos hasta obtener la capacidad de predecir lo que pasará con nuevos conjuntos de datos.\n",
        "\n",
        "Se caracterizan por disponer de una variable objetivo o variable de clase, que es justamente lo que se quiere predecir. Esta variable objetivo puede ser numérica (regresión) o categórica (clasificación)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80DnuvXQFBLZ"
      },
      "source": [
        "El **ciclo de vida** del aprendizaje supervisado consta de tres fases\n",
        "\n",
        "* **Modelamiento:** Consiste en construir el modelo que permita predecir la variable objetivo\n",
        "* **Evaluación:** Se evalúa el modelo predictivo construido para ver que tanto podemos confiar en el\n",
        "* **Validación:** Una vez el modelo ha sido evaluado y el resultado es el esperado, se someten datos nuevos al modelo para realizar la predicción\n",
        "\n",
        "La etapa de modelamiento y evaluación requieren del conjunto histórico de datos. Esto significa que debemos decidir la forma como se utilizarán los datos históricos en estas dos etapas. Para esto disponemos de tres opciones:\n",
        "\n",
        "* **Evaluar el mismo conjunto de entrenamiento**\n",
        "* **División de datos (Split / 70-30)**\n",
        "* **Validación cruzada**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1l2bA5koIvGr"
      },
      "source": [
        "###**Predicción continua o Regresión**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oxngOnmbLFa"
      },
      "source": [
        "Es el estudio de un conjunto de datos históricos con el fin de predecir un evento numérico futuro.\n",
        "\n",
        "Es decir que la variable objetivo a predecir es de tipo numérica."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "534CVcFEG5Jc"
      },
      "source": [
        "**Evaluación**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biApAd54HBC4"
      },
      "source": [
        "Como la variable a predecir es numérica el error de predicción se calcula al comparar el valor real contra el valor de la predicción y esto se hace con una fórmula matemática de cálculo de error. Existen varias formas de calcular ese error:\n",
        "\n",
        "* Error medio absoluto:\n",
        "\n",
        "$$error = \\frac{\\sum_ {i=1}^n  |f(x)-p(x)|}{n}$$\n",
        "\n",
        "* Error cuadrático medio:\n",
        "$$error = \\frac{1}{n}\\sum(f(x)-p(x))^2$$\n",
        "\n",
        "\n",
        "Donde, \n",
        "* $f(x)$: Valor real\n",
        "* $p(x)$: predicción"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEeZ2kLjKuet"
      },
      "source": [
        "####**Regresión lineal**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNaw86lWKBDG"
      },
      "source": [
        "Permite evaluar la relación entre una variable dependiente (variable a predecir) y un conjunto de variables independientes (variables predictoras)\n",
        "\n",
        "\n",
        "$$Y = \\alpha + \\beta_1 X_1  +  \\beta_2 X_2 + ... + \\beta_n X_n + \\epsilon$$\n",
        "\n",
        "Donde,\n",
        "* $Y$: Variable a predecir\n",
        "* $X$: Variables predictoras (atributos)\n",
        "* $\\alpha$: Intercepto\n",
        "* $\\beta$: Pendiente\n",
        "* $\\epsilon$: Error\n",
        "\n",
        "Básicamente lo que se hace es encontrar la ecuación de la recta que mejor represente al conjunto de datos, de modo que se pueda utilizar dicha ecuación para la predicción de nuevos registros\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2x5qVe80uHu"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIae9fVuL11u"
      },
      "source": [
        "**Ejemplo: Predicción del precio de venta de una propiedad en Boston**\n",
        "\n",
        "\n",
        "Tomado de: https://www.kaggle.com/kyasar/boston-housing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOs2SfX70rqB"
      },
      "source": [
        "**1. Preparar**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvR4JxbgMaJ7"
      },
      "source": [
        "Carga de datos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F77Y0gBmMbvJ"
      },
      "source": [
        "boston = sqlCtx.read.format(\"csv\") \\\n",
        "      .option(\"header\", True) \\\n",
        "      .option(\"delimiter\", \",\") \\\n",
        "      .option(\"inferschema\",True) \\\n",
        "      .load(\"local/data/boston_housing.csv\")\n",
        "boston.cache()\n",
        "boston.printSchema()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYJVxgeaMcpD"
      },
      "source": [
        "Esta es la descripción de las variables disponibles:\n",
        "\n",
        "* crim : Tasa de delincuencia per cápita por población\n",
        "* zn:  Proporción de terreno residencial dividido en zonas para lotes de más de 25,000 pies cuadrados.\n",
        "* indus : Proporción de terrenos comerciales no minoristas por población.\n",
        "* chas: Indica si la propiedad limita o no con el río (1 si limita con el río. 0 en caso contrario).\n",
        "* nox: Concentración de óxidos nítricos (partes por 10 millones).\n",
        "* rm : Promedio de habitaciones por vivienda.\n",
        "* age : Proporción de unidades ocupadas por sus propietarios construidas antes de 1940\n",
        "* dis : Distancias ponderadas a cinco centros de empleo de Boston\n",
        "* rad: índice de accesibilidad a carreteras principales.\n",
        "* tax : tasa de impuesto a la propiedad.\n",
        "* ptratio: Proporción alumno-profesor.\n",
        "* black: Proporción de negros por ciudad.\n",
        "* lstat: Estatus más bajo de la población (porcentaje).\n",
        "* **mv : valor medio de las viviendas en $ 1000. Esta es la variable objetivo.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9D-dqRnRwGi"
      },
      "source": [
        "Descripción estadística de los datos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TnawsXbu0fa"
      },
      "source": [
        "boston.describe().show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okGbxWfwRyf5"
      },
      "source": [
        "boston.describe().toPandas().transpose()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zd1P2umTZUXG"
      },
      "source": [
        "Análisis de correlación"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gQSZnEbZT7u"
      },
      "source": [
        "import six\n",
        "print(\"Correlación con la variable objetivo\")\n",
        "for i in boston.columns:\n",
        "    if not( isinstance(boston.select(i).take(1)[0][0], six.string_types)):\n",
        "        print( \"\", i, boston.stat.corr('medv',i))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHofhJDXqONe"
      },
      "source": [
        "Si dispone de muchas variables, se recomienda eliminar aquellas que presentan una correlación baja con la variable objetivo (<0.3)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4IfKnZPhqZ38"
      },
      "source": [
        "boston=boston.drop(\"chas\",\"dis\")\n",
        "boston.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4986Z6afumq"
      },
      "source": [
        "Para aplicar la técnica de Machine Learning se requiere representar los datos mediante dos columnas, una con las variables predictoras (features) y la otra es la variable objetivo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKJrFUwNgBTq"
      },
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "vectorAssembler = VectorAssembler(inputCols = \\\n",
        "    ['crim', 'zn', 'indus', 'nox', 'rm', 'age', 'rad', 'tax', 'ptratio', 'black', 'lstat'], outputCol = 'features')\n",
        "vboston = vectorAssembler.transform(boston)\n",
        "vboston.show(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHk_uSKogu-z"
      },
      "source": [
        "Nos quedamos solo con las variables features y medv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8IAa6PRg3cJ"
      },
      "source": [
        "vboston = vboston.select(['features', 'medv'])\n",
        "vboston.show(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MoZJ52uKsRsV"
      },
      "source": [
        "Ahora veamos la correlación entre las variables predictoras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvRlcKb2s15L"
      },
      "source": [
        "matrix = Correlation.corr(vboston,\"features\")\n",
        "matrix.collect()[0][\"pearson({})\".format(\"features\")].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8fBkXrBwlQv"
      },
      "source": [
        "print(\"Correlación entre tax y indus: \",boston.corr(\"tax\",\"indus\"))\n",
        "print(\"Correlación entre nox y indus: \",boston.corr(\"nox\",\"indus\"))\n",
        "print(\"Correlación entre nox y age: \",  boston.corr(\"nox\",\"age\"))\n",
        "print(\"Correlación entre tax y rad: \",  boston.corr(\"tax\",\"rad\"))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPgxJpIT0c9L"
      },
      "source": [
        "Eliminemos las variables correlacionadas y creemos nuevamente el vector de características"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oj-MoN-lzoNP"
      },
      "source": [
        "boston=boston.drop(\"tax\",\"nox\")\n",
        "vectorAssembler = VectorAssembler(inputCols = \\\n",
        "    ['crim', 'zn', 'indus', 'rm', 'age', 'rad', 'ptratio', 'black', 'lstat'], outputCol = 'features')\n",
        "vboston = vectorAssembler.transform(boston)\n",
        "vboston = vboston.select(['features', 'medv'])\n",
        "vboston.show(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJmyxSf3hXc5"
      },
      "source": [
        "Creamos los conjuntos de datos de entrenamiento y evaluación (Split)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "If4wlrfChc4B"
      },
      "source": [
        "(trainingData, testData) = vboston.randomSplit([0.7, 0.3])\n",
        "print(trainingData.count())\n",
        "print(testData.count())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LR8YXzbChxYa"
      },
      "source": [
        "**2. Modelar**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMUDLzOyh0Ro"
      },
      "source": [
        "from pyspark.ml.regression import LinearRegression\n",
        "\n",
        "lr = LinearRegression(featuresCol = 'features', labelCol='medv', maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
        "lr_model = lr.fit(trainingData)\n",
        "trainingSummary = lr_model.summary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4o-FVN83AG6"
      },
      "source": [
        "Veamos la predicción con los datos de entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYyS8Bem2s0j"
      },
      "source": [
        "trainingSummary.predictions.select(\"prediction\",\"medv\").show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2S6RfkM3R3x"
      },
      "source": [
        "El método de regresión lineal construye la ecuación de la recta que mas se ajuste a los datos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6YELOVs3hPC"
      },
      "source": [
        "print(\"Coeficientes: \" + str(lr_model.coefficients))\n",
        "print(\"Intercepto: \" + str(lr_model.intercept))\n",
        "print(\"Variables:\")\n",
        "boston.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jQVLTjUipvo"
      },
      "source": [
        "Veamos las medidas de error"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZopfZ3ahizox"
      },
      "source": [
        "print(\"Error cuadrático medio (MSE): %f\" % trainingSummary.meanSquaredError)\n",
        "print(\"Error medio absoluto (MAE): %f\" % trainingSummary.meanAbsoluteError)\n",
        "print(\"Raíz del Error cuadrático medio (RMSE): %f\" % trainingSummary.rootMeanSquaredError)\n",
        "print(\"Coeficiente de determinación (r2): %f\" % trainingSummary.r2)\n",
        "print(\"Coeficiente de determinación ajustado (r2Adj): %f\" % trainingSummary.r2adj)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mdlkUZXjwSg"
      },
      "source": [
        "\n",
        "**3. Evaluar**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87rtcQGUj3af"
      },
      "source": [
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "lr_predictions = lr_model.transform(testData)\n",
        "lr_predictions.select(\"prediction\",\"medv\").show(5)\n",
        "\n",
        "lr_evaluator = RegressionEvaluator(predictionCol=\"prediction\", \\\n",
        "                 labelCol=\"medv\",metricName=\"r2\")\n",
        "print(\"Coeficiente de determinación con los datos de evaluación = %g\" % lr_evaluator.evaluate(lr_predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhd7H-FnlMnH"
      },
      "source": [
        "test_result = lr_model.evaluate(testData)\n",
        "print(\"Error cuadrático medio con los datos de evaluación = %g\" % test_result.meanSquaredError)\n",
        "print(\"Raiz del Error cuadrático medio con los datos de evaluación = %g\" % test_result.rootMeanSquaredError)\n",
        "print(\"Error medio absoluto con los datos de evaluación = %g\" % test_result.meanAbsoluteError)\n",
        "print(\"Coeficiente de determinación con los datos de evaluación = %g\" % test_result.r2)\n",
        "print(\"Coeficiente de determinación ajustado con los datos de evaluación = %g\" % test_result.r2adj)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htUKk--PK3CC"
      },
      "source": [
        "####**Árboles de decisión para regresión**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAS-JanfJIL_"
      },
      "source": [
        "El método de árboles de decisión está disponible tanto para predecir números (árboles de regresión) como para predecir categorías (árboles de clasificación).\n",
        "\n",
        "Su funcionamiento se basa en representar el conjunto histórico de datos a través de un árbol en el que tendremos dos componentes:\n",
        "* Nodos: presentan una pregunta sobre algunos de los atributos y se genera una bifurcación a partir de la respuesta\n",
        "* Hojas: Están al final de cada rama y son los valores de la predicción\n",
        "\n",
        "\n",
        "Cuando se somete un registro al árbol de decisión, este recorre el árbol a través de las respuestas generadas en cada nodo, y al final llegará a una hoja que indica la predicción.\n",
        "\n",
        "En el caso de los árboles de regresión, la variable a predecir es numérica, es decir que el resultado final de la predicción será un número "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IuFzl67m05Q"
      },
      "source": [
        "**Ejemplo: Predicción del precio de venta de una propiedad en Boston**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVnwpGsVhA-b"
      },
      "source": [
        "**Modelar**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82lQn7ymm-2L"
      },
      "source": [
        "from pyspark.ml.regression import DecisionTreeRegressor\n",
        "dt = DecisionTreeRegressor(featuresCol ='features', labelCol = 'medv')\n",
        "dt_model = dt.fit(trainingData)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0b5FQby14rDi"
      },
      "source": [
        "Veamos la predicción con los datos de entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6I8D6iI4sza"
      },
      "source": [
        "dt_model.transform(trainingData).select(\"prediction\",\"medv\").show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRdmIiA7hDGO"
      },
      "source": [
        "**Evaluar**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WoC66bphEb_"
      },
      "source": [
        "dt_predictions = dt_model.transform(testData)\n",
        "dt_predictions.select(\"prediction\",\"medv\").show(5)\n",
        "\n",
        "dt_evaluator = RegressionEvaluator(\n",
        "    labelCol=\"medv\", predictionCol=\"prediction\", metricName=\"r2\")\n",
        "print(\"Coeficiente de determinación con los datos de evaluación = %g\" % dt_evaluator.evaluate(dt_predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OkygbOPnm4v"
      },
      "source": [
        "Veamos la importancia de las vaiables predictoras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5YSvSUsnwlC"
      },
      "source": [
        "dt_model.featureImportances"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3a1THHbAn6Tu"
      },
      "source": [
        "boston.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2wdD-W2Irqj"
      },
      "source": [
        "###**Predicción discreta o Clasificación**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WG00ebF2bNom"
      },
      "source": [
        "La clasificación permite predecir un evento numérico discreto, es decir que lo que pedimos ya no es un número sino una categoría, una cualidad.\n",
        "\n",
        "Se utiliza el conjunto histórico de datos donde se cuenta con una variable objetivo que es de tipo categórica. Con ese histórico de datos, se crea un modelo que aprenderá de los datos y permitirá predecir datos futuros"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7ujBIeGbd-2"
      },
      "source": [
        "**Evaluación**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5WMsOpqVhAf"
      },
      "source": [
        "Teniendo en cuenta que la variable objetivo es categórica, contamos con la clase real a la que pertenece cada uno de los registros que hacen parte del histórico de datos.\n",
        "\n",
        "Por otro lado, el modelo arrojará como resultado una predicción sobre la clase a la que debe pertenecer cada registro. Usando la clase real y la predicción se construyen unas medidas de error que permiten evaluar la calidad del modelo. \n",
        "\n",
        "Estas medidas se construyen al identificar la cantidad de registros que quedaron clasificados correctamente es decir que la clase de la predicción y la clase real coinciden\n",
        "\n",
        "Con esta información se construye lo que se conoce como matriz de confusión\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6bWD3OyYk29"
      },
      "source": [
        "Image(\"local/imgs/matriz_confusion.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnCt3hbMYy2w"
      },
      "source": [
        "las cantidades relacionadas en la matriz de confusión nos permiten calcular esas medidas de error entre las cuales se destacan:\n",
        "\n",
        "* Precisión: $$p=\\frac{a}{a+b}$$\n",
        "* Cobertura: $$r=\\frac{a}{a+c}$$\n",
        "* Exactitud: $$e=\\frac{a+d}{a+b+c+d}$$\n",
        "* Media armónica: $$f=\\frac{2pr}{p+r}$$\n",
        "* Razón de verdaderos positivos: $$VPR=\\frac{a}{a+c}$$\n",
        "* Razón de falsos positivos: $$FPR=\\frac{b}{b+d}$$\n",
        "\n",
        "Donde,\n",
        "\n",
        "* $a$: Verdaderos positivos\n",
        "* $b$: Falsos positivos\n",
        "* $c$: Falsos negativos\n",
        "* $d$: Verdaderos negativos\n",
        "\n",
        "Con la razón de verdaderos positivos y la razón de falsos positivos se calcula el área ROC, medida utilizada ampliamente para evaluar modelos predictivos de clasificación.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OomSEZ1_LX4r"
      },
      "source": [
        "####**Regresión Logística**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZ4UOWyIbQPO"
      },
      "source": [
        "El funcionamiento es similar a la regresión lineal, donde se calculaba la ecuación de la recta que mejor represente al conjunto de datos con el fin de predecir una variable numérica.\n",
        "\n",
        "La regresión logística aplica ese principio, pero teniendo en cuenta que la variable a predecir es categórica y puede tener n categorías,\n",
        "lo que propone es calcular las ecuaciones de n rectas, una para cada categoría "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lo0KZGMJr4if"
      },
      "source": [
        "**Ejemplo: Aprobación curso**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68sZIwD0sIGi"
      },
      "source": [
        "**1. Preparar**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMpNMkfSo4li"
      },
      "source": [
        "data = pd.read_excel(\"local/data/aprobacion_curso_bal.xlsx\",sheet_name=0)\n",
        "estDF=sqlCtx.createDataFrame(data).drop('Unnamed: 0')\n",
        "estDF.show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hFpn-PDr4Jf"
      },
      "source": [
        "estDF=sqlCtx.createDataFrame(data)\n",
        "estDF=estDF.\\\n",
        "  withColumn('equilibrio',(estDF.Felder=='equilibrio').cast(IntegerType())).\\\n",
        "  withColumn('activo',(estDF.Felder=='activo').cast(IntegerType())).\\\n",
        "  withColumn('visual',(estDF.Felder=='visual').cast(IntegerType())).\\\n",
        "  withColumn('reflexivo',(estDF.Felder=='reflexivo').cast(IntegerType())).\\\n",
        "  withColumn('sensorial',(estDF.Felder=='sensorial').cast(IntegerType())).\\\n",
        "  withColumn('intuitivo',(estDF.Felder=='intuitivo').cast(IntegerType())).\\\n",
        "  withColumn('secuencial',(estDF.Felder=='secuencial').cast(IntegerType())).\\\n",
        "  withColumn('verbal',(estDF.Felder=='verbal').cast(IntegerType())).\\\n",
        "  withColumn('AproboInt',(estDF.Aprobo=='si').cast(IntegerType()))\n",
        "estDF=estDF.drop('Felder','Aprobo','Unnamed: 0')\n",
        "estDF.printSchema()\n",
        "estDF.show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wk3qGd8Wv_Zn"
      },
      "source": [
        "vectorAssembler = VectorAssembler(inputCols = \\\n",
        "    ['Examen_admisión', 'equilibrio', 'activo', 'visual', 'reflexivo', 'sensorial', 'intuitivo', \\\n",
        "     'secuencial', 'verbal'], outputCol = 'features')\n",
        "vest = vectorAssembler.transform(estDF)\n",
        "vest = vest.select(['features', 'AproboInt'])\n",
        "vest.show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgqVcUwXv32e"
      },
      "source": [
        "Creamos los conjuntos de datos de entrenamiento y evaluación (Split)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snd9vjrUpKV1"
      },
      "source": [
        "(trainingData, testData) = vest.randomSplit([0.7, 0.3])\n",
        "print(trainingData.count())\n",
        "print(testData.count())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yq1qWnpfs8WP"
      },
      "source": [
        "**2. Modelar**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ToOd12tGpVVe"
      },
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "lr = LogisticRegression(labelCol=\"AproboInt\", featuresCol=\"features\",maxIter=10)\n",
        "lr_model=lr.fit(trainingData)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "noGeb7_4HGdi"
      },
      "source": [
        "trainingSummary = lr_model.summary\n",
        "trainingSummary.predictions.select('AproboInt','prediction','probability').show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IDsniXIGVSj"
      },
      "source": [
        "print(\"Coeficientes: \" + str(lr_model.coefficients))\n",
        "print(\"Intercepto: \" + str(lr_model.intercept))\n",
        "estDF.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJfBZjXXIQzT"
      },
      "source": [
        "Veamos el área ROC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BnWjv3sMH4na"
      },
      "source": [
        "roc = trainingSummary.roc.toPandas()\n",
        "plt.plot(roc['FPR'],roc['TPR'])\n",
        "plt.ylabel('Razón de los falsos positivos')\n",
        "plt.xlabel('Razón de los verdaderos positivos')\n",
        "plt.title('Curva ROC')\n",
        "plt.show()\n",
        "print('Área ROC con los datos de entrenamiento: ' + str(trainingSummary.areaUnderROC))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwtdMZXGyQFC"
      },
      "source": [
        "**3. Evaluar**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aw7m92CzyVqz"
      },
      "source": [
        "predict_test=lr_model.transform(testData)\n",
        "predict_test.select('AproboInt','prediction','probability').show(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szUpbCfyJoOZ"
      },
      "source": [
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "evaluator = BinaryClassificationEvaluator(labelCol='AproboInt')\n",
        "print('Área ROC con los datos de evaluación', evaluator.evaluate(predict_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXMCOXfpLRaa"
      },
      "source": [
        "####**Árboles de decisión para clasificación**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvbAuvUBbRWK"
      },
      "source": [
        "El funcionamiento es similar a los árboles de regresión, solo que en este caso la variable a predecir es categórica.\n",
        "\n",
        "A partir del conjunto histórico de datos se construye en árbol que tiene en sus nodos una pregunta sobre alguno de los atributos y en las hojas alguna de las categorías de la variable objetivo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPxJRJ0_Livb"
      },
      "source": [
        "**Modelar**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXZv6cqoLnI4"
      },
      "source": [
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "dt = DecisionTreeClassifier(labelCol=\"AproboInt\", featuresCol=\"features\")\n",
        "dt_model=dt.fit(trainingData)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLL356u8Oe5S"
      },
      "source": [
        "predict_train = dt_model.transform(trainingData)\n",
        "predict_train.select('AproboInt','prediction').show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuIFRfiwLlZf"
      },
      "source": [
        "**Evaluar**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0qD-p-hPOAx"
      },
      "source": [
        "predict_test = dt_model.transform(testData)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgmG_702O3MD"
      },
      "source": [
        "Veamos la precisión del clasificador"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFCfqnIjN6P_"
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "clase_real = predict_test.select(['AproboInt']).collect()\n",
        "clase_prediccion = predict_test.select(['prediction']).collect()\n",
        "\n",
        "print(\"Medidas de error\")\n",
        "print(classification_report(clase_real, clase_prediccion))\n",
        "print(\"Matriz de confusión\")\n",
        "print(confusion_matrix(clase_real, clase_prediccion))\n",
        "evaluator = BinaryClassificationEvaluator(labelCol='AproboInt')\n",
        "print('Área ROC con los datos de evaluación', evaluator.evaluate(predict_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-DgcQ9MLmI7"
      },
      "source": [
        "####**Redes neuronales**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1XyrwXf1bhUP"
      },
      "source": [
        "Las redes neuronales son modelos computacionales inspirados en sistemas biológicos, adaptados y simulados en computadoras convencionales. Buscan simular en un algoritmo la forma como se comportan las neuronas del cerebro humano.\n",
        "\n",
        "Cada neurona tiene variables de entrada (atributos) que son multiplicados por un vector de pesos y el resultado es llevado a un mezclador lineal. A la salida del mezclador lineal se aplica una función de activación que puede ser tan sencilla o compleja como se quiera, el resultado es la salida de la neurona.\n",
        "\n",
        "Las neuronas no trabajan solas, los resultados de cada neurona pueden ser llevados a varias neuronas que los utilizan como entradas. En este caso diremos que la red neuronal puede tener varias capas y dentro de cada capa se puede tener varias neuronas "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxPc7XDJ4Gnm"
      },
      "source": [
        "**Multilayer Perceptron**\n",
        "\n",
        "Es una red neuronal con alimentación hacia adelante, compuesta de varias capas de neuronas entre la entrada y la salida de la misma, esta red permite establecer regiones de decisión mucho más complejas que las de dos semiplanos.\n",
        "\n",
        "Como **función de activación** puede utilizar:\n",
        "* Hardlim: Función escalón entre 0 y 1\n",
        "* Hardlims: Función escalón simétrica entre -1 y 1\n",
        "\n",
        "Multilayer perceptron utiliza la siguiente **regla de aprendizaje**:\n",
        "* Pesos: $$w_{i+1} = w_i + (y_i - y_i')x_i $$\n",
        "* Variable independiente: $$b_{i+1} = b_i + (y_i - y_i')$$\n",
        "\n",
        "Donde,\n",
        "\n",
        "* $w_{i+1}$: Peso de la neurona en la siguiente iteración\n",
        "* $w_i$: Peso de la neurona en la iteración actual\n",
        "* $y_i$: Salida real\n",
        "* $y_i'$: Predicción\n",
        "* $x_i$: Entrada actual de la neurona\n",
        "* $b_{i+1}$: Entrada independiente en la siguiente iteración\n",
        "* $b_i$: Entrada independiente en la iteración actual\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beLwH3_3ZfX_"
      },
      "source": [
        "**Modelar**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lnk6_E1MZg3y"
      },
      "source": [
        "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
        "nn= MultilayerPerceptronClassifier(labelCol='AproboInt', featuresCol='features', maxIter=100, layers=[9, 5, 2])\n",
        "nn_model = nn.fit(trainingData)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPqI0UKQavZF"
      },
      "source": [
        "predict_train = nn_model.transform(trainingData)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TY6TuUg2Z0IY"
      },
      "source": [
        "**Evaluar**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYau9aK-Z1uk"
      },
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "#Predicción\n",
        "predict_test = nn_model.transform(testData)\n",
        "predict_test.select('prediction', 'AproboInt').show(5)\n",
        "\n",
        "#Exactitud\n",
        "evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\", labelCol='AproboInt')\n",
        "print(\"Exactitud con los datos de evaluación = \" + str(evaluator.evaluate(predict_test)))\n",
        "\n",
        "#Matriz de confusión\n",
        "clase_real = predict_test.select(['AproboInt']).collect()\n",
        "clase_prediccion = predict_test.select(['prediction']).collect()\n",
        "print(\"Matriz de confusión\")\n",
        "print(confusion_matrix(clase_real, clase_prediccion))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HumWu-FwLyRH"
      },
      "source": [
        "####**Máquinas de Soporte Vectorial**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7htFFfwkbhw3"
      },
      "source": [
        "Las máquinas de soporte vectorial plantean la solución de un problema complejo (no separable linealmente) en un espacio de dimensión superior mediante la construcción de un hiperplano de separación óptima de clases.\n",
        "\n",
        "El hiper plano construido permite separar a cada lado del mismo los registros de cada categoría de la variable objetivo.\n",
        "\n",
        "Para representar los datos en una dimensión superior se utiliza la matriz de Kernel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPoAVsbtq2le"
      },
      "source": [
        "**Modelar**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iA5rHQ9Zq59A"
      },
      "source": [
        "from pyspark.ml.classification import LinearSVC\n",
        "svc= LinearSVC(labelCol='AproboInt', featuresCol='features', maxIter=10, regParam=0.1)\n",
        "svc_model = svc.fit(trainingData)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jeKq6UL0rclb"
      },
      "source": [
        "predict_train = svc_model.transform(trainingData)\n",
        "predict_train.select('AproboInt', 'prediction').show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0E5i69cq4Jy"
      },
      "source": [
        "**Evaluar**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTS_CV2Jq60h"
      },
      "source": [
        "predict_test = svc_model.transform(testData)\n",
        "predict_test.select('prediction', 'AproboInt').show(5)\n",
        "evaluator = BinaryClassificationEvaluator(labelCol='AproboInt')\n",
        "print('Área ROC con los datos de evaluación', evaluator.evaluate(predict_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhFiMocQMANZ"
      },
      "source": [
        "####**Método Bayesiano**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBemkPu1biUH"
      },
      "source": [
        "Los métodos bayesianos utilizan el teorema de la probabilidad condicional de bayes: La probabilidad de que algo ocurra dado que ya han ocurrido ciertos sucesos.\n",
        "\n",
        "La probabilidad de que un registro $reg$ pertenezca a la clase $C_j$ se calcula como:\n",
        "$$P(C_j/reg) = \\frac{P(C_j)P(reg/C_j)}{P(reg)}$$\n",
        "\n",
        "Donde,\n",
        "\n",
        "* $P(C_j)$: Probabilidad de la clase $C_j$\n",
        "* $P(reg/C_j)$: Probabilidad de que la clase $C_j$ sea la clase del registro $reg$\n",
        "* $P(reg)$: Probabilidad de seleccionar un registro\n",
        "\n",
        "Naive Bayes implementa un método bayesiano asumiendo que los registros son independientes, con esto la probabilidad de que un registro $reg$ pertenezca a la clase $C_j$ se resume como:\n",
        "\n",
        "$$P(C_j/reg) = P(C_j)P(reg/C_j)$$\n",
        "\n",
        "\n",
        "Con los datos históricos se construye una tabla de probabilidades, que indica la probabilidad de que un registro pertenezca a cada una de las categorías de la variable objetivo\n",
        "\n",
        "Dado un registro nuevo, la predicción consistirá en evaluar la probabilidad de pertenencia de dicho registro a cada una de las clases escogiendo aquella que cuente con la mayor probabilidad."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3ficNVDry3U"
      },
      "source": [
        "**Modelar**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fWs-lCAr0QE"
      },
      "source": [
        "from pyspark.ml.classification import NaiveBayes\n",
        "nb= NaiveBayes(labelCol='AproboInt', featuresCol='features')\n",
        "nb_model = svc.fit(trainingData)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAlauXJzsed2"
      },
      "source": [
        "predict_train = nb_model.transform(trainingData)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8xoewLCr0tk"
      },
      "source": [
        "**Evaluar**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPD6KY6tr2Jm"
      },
      "source": [
        "predict_test = nb_model.transform(testData)\n",
        "predict_test.select('prediction', 'AproboInt').show(5)\n",
        "evaluator = BinaryClassificationEvaluator(labelCol='AproboInt')\n",
        "print('Área ROC con los datos de evaluación', evaluator.evaluate(predict_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkOAFatYIi4d"
      },
      "source": [
        "##**Aprendizaje NO Supervisado**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7rri9sabj74"
      },
      "source": [
        "En el aprendizaje No Supervisado, NO se cuenta con datos históricos previamente etiquetados para la etapa de entrenamiento, en su lugar se dispone de datos actuales y lo que se haces es describir la estructura de esos datos actuales mediante un análisis exploratorio con el fin de facilitar el entendimiento de los datos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAu47BlDI2WC"
      },
      "source": [
        "###**Clustering**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4scVGdpEUlnN"
      },
      "source": [
        "Su objetivo es agrupar un conjunto de datos heterogéneo en grupos de datos homogéneos. En principio cada registro (fila) del conjunto de datos es diferente de los demás, por eso decimos que es heterogéneo, pero puede tener cierta similitud a un subconjunto de registros y es ahi donde se genera la agrupación, siendo cada subconjunto de datos (clúster) diferente a los demás subconjuntos.\n",
        "\n",
        "La similaridad de los registros se expresa como una medida de distancia, de modo que un par de registros que se encuentre distante implicará que son diferentes, mientras que registros cercanos diremos que son similares\n",
        "\n",
        "Dentro del **ciclo de vida** del Clustering se encuentran las siguientes etapas:\n",
        "* Aprendizaje: Crear el modelo\n",
        "* Evaluación: Evaluar el modelo\n",
        "* Perfilamiento: Entender las características que definen a cada clúster (describir el centroide)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uySx0guebldW"
      },
      "source": [
        "**Evaluación**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhfAuICObpkW"
      },
      "source": [
        "La evaluación del Clustering se basa en la comparación de dos medidas, estas medidas son:\n",
        "* Cohesión o compacticidad: promedio de distancia de cada elemento a su centroide (distancia dentro del clúster)\n",
        "* Separabilidad: promedio de distancias entre los centroides de los clústers (distancia entre clústers)\n",
        "\n",
        "La comparación de estas dos medidas se expresa mediante índices, los mas conocidos son:\n",
        "\n",
        "* Dunn\n",
        "* Davies-Bouldin\n",
        "* Silueta\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnTZI0tnbs-W"
      },
      "source": [
        "####**K-means**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zN0UkWWpUmtc"
      },
      "source": [
        "Divide el conjunto de datos en un número predefinido de grupos k. Es el método más comúnmente utilizado, la idea del método es definir k centroides, uno por clúster, y los datos son asociados al centroide más cercano.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fy1PzX0Sugj0"
      },
      "source": [
        "**Ejemplo: Titanic**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quO00lSgunfw"
      },
      "source": [
        "**Preparar**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNqWRnzGutty"
      },
      "source": [
        "data = sqlCtx.read.option(\"header\",True) \\\n",
        "      .option(\"delimiter\",\";\") \\\n",
        "     .csv(\"local/data/titanic.csv\")\n",
        "data.show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDtbtp3vyO4_"
      },
      "source": [
        "def func_Clase(value):\n",
        "  if value==\"Tripulacion\":\n",
        "    return 0\n",
        "  if value==\"Primera\":\n",
        "    return 1\n",
        "  if value==\"Segunda\":\n",
        "    return 2\n",
        "  if value==\"Tercera\":\n",
        "    return 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GhUyhgE5dai"
      },
      "source": [
        "def func_Edad(value):\n",
        "  if value==\"Adulto\":\n",
        "    return 1\n",
        "  else:\n",
        "    return 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jL1lTiEo5ufZ"
      },
      "source": [
        "def func_Sexo(value):\n",
        "  if value==\"Hombre\":\n",
        "    return 1\n",
        "  else:\n",
        "    return 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBFfU0IV55oF"
      },
      "source": [
        "def func_Sobrevivio(value):\n",
        "  if value==\"Si\":\n",
        "    return 1\n",
        "  else:\n",
        "    return 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIFCtzKRz4qy"
      },
      "source": [
        "data.registerTempTable(\"Titanic\")\n",
        "sqlCtx.registerFunction(\"func_clase\",func_Clase)\n",
        "sqlCtx.registerFunction(\"func_edad\",func_Edad)\n",
        "sqlCtx.registerFunction(\"func_sexo\",func_Sexo)\n",
        "sqlCtx.registerFunction(\"func_sobrevivio\",func_Sobrevivio)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0K7pvt1U0U5X"
      },
      "source": [
        "titanic = sqlCtx.sql(\"\"\"\n",
        "\t\tSELECT func_clase(Clase) AS ClaseInt, func_edad(Edad) AS EdadInt, \n",
        "      func_sexo(Sexo) AS SexoInt, func_sobrevivio(Sobrevivio) AS SobrevivioInt\n",
        "\t\tFROM Titanic\n",
        "\t\t\"\"\")\n",
        "titanic.distinct().show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pG3l3u0Q8sqq"
      },
      "source": [
        "titanic.printSchema()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8bbF9cK-D9f"
      },
      "source": [
        "titanic = titanic.withColumn(\"ClaseInt\", titanic[\"ClaseInt\"].cast(IntegerType()))\n",
        "titanic = titanic.withColumn(\"EdadInt\", titanic[\"EdadInt\"].cast(IntegerType()))\n",
        "titanic = titanic.withColumn(\"SexoInt\", titanic[\"SexoInt\"].cast(IntegerType()))\n",
        "titanic = titanic.withColumn(\"SobrevivioInt\", titanic[\"SobrevivioInt\"].cast(IntegerType()))\n",
        "data=titanic\n",
        "data.printSchema()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dle0mPOo8YZ4"
      },
      "source": [
        "vectorAssembler = VectorAssembler(inputCols = data.columns, outputCol = 'features', handleInvalid=\"skip\")\n",
        "vdata = vectorAssembler.transform(data)\n",
        "vdata.cache()\n",
        "vdata.show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_9n-pKvupxT"
      },
      "source": [
        "**Modelar**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJ_FIAUkuugS"
      },
      "source": [
        "from pyspark.ml.clustering import KMeans\n",
        "km = KMeans( featuresCol='features', k=3, predictionCol='cluster', distanceMeasure='euclidean')\n",
        "km_model = km.fit(vdata)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhmkWRqyBTwg"
      },
      "source": [
        "Veamos en cuál cluster quedó cada registro"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLHOA8ep_ULv"
      },
      "source": [
        "predictions = km_model.transform(vdata)\n",
        "predictions.distinct().show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nOAsUXqBiJQ"
      },
      "source": [
        "Veamos los centroides de cada cluster"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iM2tYnNLJIq9"
      },
      "source": [
        "centers = km_model.clusterCenters()\n",
        "print(\"Centroides: \")\n",
        "for center in centers:\n",
        "  centerR=[round(num, 0) for num in center]\n",
        "  print(centerR)\n",
        "print(\"Variables\")\n",
        "data.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dvjBLmDurtA"
      },
      "source": [
        "**Evaluar**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KW1LEXNuvRH"
      },
      "source": [
        "from pyspark.ml.evaluation import ClusteringEvaluator\n",
        "evaluator = ClusteringEvaluator(predictionCol='cluster')\n",
        "silhouette = evaluator.evaluate(predictions)\n",
        "print(\"Índice de la silueta = \" + str(silhouette))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3VdL2fVKpv6"
      },
      "source": [
        "Aumentemos el número de clústers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJbBl8okKwlE"
      },
      "source": [
        "#modelar\n",
        "km = KMeans( featuresCol='features', k=18, predictionCol='cluster', distanceMeasure='euclidean')\n",
        "km_model = km.fit(vdata)\n",
        "\n",
        "#Asignación de clusters\n",
        "predictions = km_model.transform(vdata)\n",
        "\n",
        "#Evaluar\n",
        "evaluator = ClusteringEvaluator(predictionCol='cluster')\n",
        "silhouette = evaluator.evaluate(predictions)\n",
        "print(\"Índice de la silueta = \" + str(silhouette))\n",
        "\n",
        "#Centroides\n",
        "centers = km_model.clusterCenters()\n",
        "print(\"Centroides: \")\n",
        "for center in centers:\n",
        "  centerR=[round(num, 0) for num in center]\n",
        "  print(centerR)\n",
        "print(\"Variables\")\n",
        "data.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOKJVcNEXktB"
      },
      "source": [
        "Cómo podemos saber cuántos clúter deberíamos crear?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oR6fXpSIZ654"
      },
      "source": [
        "from pyspark.ml.clustering import KMeans\n",
        "kini=2; kfin=20\n",
        "ks = range(kini, kfin)\n",
        "costo=[]\n",
        "for k in range(kini, kfin):\n",
        "    kmeans= KMeans( featuresCol='features', k=k, predictionCol='cluster', distanceMeasure='euclidean')\n",
        "    model = kmeans.fit(vdata)\n",
        "    costo.append(model.summary.trainingCost)\n",
        "plt.plot(ks, costo, '-o')\n",
        "plt.title('Método del codo')\n",
        "plt.xlabel('Número de clusters')\n",
        "plt.ylabel('costo')\n",
        "plt.xticks(ks)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1E_j_gOITebU"
      },
      "source": [
        "#**Ejemplos**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HeDPSuNSbHA"
      },
      "source": [
        "##**Filtrado Colaborativo**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZtSMI5Fcywm"
      },
      "source": [
        "Recomendación de películas a partir de la calificación ingresada por los usuarios"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHfilM4pm7kt"
      },
      "source": [
        "**Preparar**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11qJzcc1d3vy"
      },
      "source": [
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.ml.recommendation import ALS\n",
        "lines = sc.textFile(\"local/data/movielens.txt\")\n",
        "parts = lines.map(lambda row: row.split(\"::\"))\n",
        "ratingsRDD = parts.map(lambda p: (int (p[0]), int (p[1]), float (p[2])))\n",
        "ratingsRDD.take(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irNwwWEKfnVt"
      },
      "source": [
        "ratings = sqlCtx.createDataFrame(ratingsRDD, [\"usuario\",\"pelicula\",\"calificacion\"])\n",
        "(training, test) = ratings.randomSplit([0.7, 0.3])\n",
        "training.show(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qz_yf60Fm99a"
      },
      "source": [
        "**Modelar**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IrblymPTeLpI"
      },
      "source": [
        "als = ALS(maxIter=5, regParam=0.01, userCol=\"usuario\", itemCol=\"pelicula\", ratingCol=\"calificacion\",\n",
        "          coldStartStrategy=\"drop\")\n",
        "model = als.fit(training)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WDmGrW9nJbr"
      },
      "source": [
        "**Evaluar**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RI99BwFmf28j"
      },
      "source": [
        "predictions = model.transform(test)\n",
        "predictions.show(5)\n",
        "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"calificacion\",\n",
        "                                predictionCol=\"prediction\")\n",
        "rmse = evaluator.evaluate(predictions)\n",
        "print(\"Raiz del error cuadrático medio = \" + str(rmse))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vKr69NFkpWC"
      },
      "source": [
        "**Películas recomendadas para cada usuario**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXJQZ8yoeLYk"
      },
      "source": [
        "userRecs = model.recommendForAllUsers(2)\n",
        "userRecs.take(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6xscFU8kvvK"
      },
      "source": [
        "**Usuarios recomendados para cada película**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9lhmTz3Hizc0"
      },
      "source": [
        "movieRecs = model.recommendForAllItems(2)\n",
        "movieRecs.take(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oIcsGarpAaR"
      },
      "source": [
        "**Películas recomendadas para una lista de usuarios específica**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "847dvGJGluNv"
      },
      "source": [
        "users = ratings.select(als.getUserCol()).distinct().limit(3)\n",
        "userSubsetRecs = model.recommendForUserSubset(users, 5)\n",
        "userSubsetRecs.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmZXqZ71peEx"
      },
      "source": [
        "**Usuarios recomendados para una lista de películas específica**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SL4NDphai0Fs"
      },
      "source": [
        "movies = ratings.select(als.getItemCol()).distinct().limit(3)\n",
        "movieSubSetRecs = model.recommendForItemSubset(movies, 5)\n",
        "movieSubSetRecs.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXmwr-0BTa3V"
      },
      "source": [
        "#**Ejercicios**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tArlRjDXs5Vz"
      },
      "source": [
        "**Población atendida en hospital de Envigado**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjsvuwR0M60m"
      },
      "source": [
        "El archivo poblacion_atendida.csv contiene información sobre los pacientes atendidos en un Hospital en Envigado. Para cada paciente se tiene la siguiente información:\n",
        "\n",
        "* SEXO\t\n",
        "* ESTADO_CIVIL\t\n",
        "* ZONA_RES\t\n",
        "* MUN_RES\t\n",
        "* PAIS_NCTO\t\n",
        "* EDAD\t\n",
        "* ESCOLARIDAD_PACIENTE\n",
        "\n",
        "Fuente de los datos:\n",
        "https://www.datos.gov.co/Salud-y-Protecci-n-Social/Poblaci-n-Atendida-a-o-2020/4ike-xz34"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeGenHNMOlDf"
      },
      "source": [
        "A) Aplique Kmeans para crear clústers que permitan describir  los tipos de pacientes "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgVo1qxpO_Kj"
      },
      "source": [
        "B) Usando el clúster asignado a cada paciente como variable objetivo, cree un modelo predictivo que permita predecir el grupo al que pertenece cada paciente"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kv2cX1XTPYmz"
      },
      "source": [
        "C) Cree un archivo con la información de tres nuevos pacientes y utilice el modelo de predicción creado anteriormente para predecir el clúster al que pertenecerían los nuevos pacientes"
      ]
    }
  ]
}